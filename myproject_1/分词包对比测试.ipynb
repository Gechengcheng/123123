{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import gensim\n",
    "from snownlp import SnowNLP\n",
    "import thulac\n",
    "# 百度分词器\n",
    "from LAC import LAC\n",
    "# 装载分词模型\n",
    "lac = LAC()\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结巴分词器:\n",
      " 大会 的 主题 是 ： 不忘 初心 ， 牢记 使命 ， 高举 中国 特色 社会主义 伟大旗帜 ， 决胜 全面 建成 小康社会 ， 夺取 新 时代 中国 特色 社会主义 伟大胜利 ， 为 实现 中华民族 伟大 复兴 的 中国 梦 不懈 奋斗\n",
      "百度分词器:\n",
      " 大会 的 主题 是 ： 不忘初心 ， 牢记 使命 ， 高举 中国特色社会主义 伟大 旗帜 ， 决胜 全面建成小康社会 ， 夺取 新时代中国特色社会主义 伟大 胜利 ， 为 实现 中华民族 伟大 复兴 的 中国梦 不懈 奋斗\n",
      "snownlp分词器:\n",
      " 大会 的 主题 是 ： 不 忘 初心 ， 牢记 使命 ， 高举 中国 特色 社会主义 伟大 旗帜 ， 决胜 全面 建成 小康 社会 ， 夺取 新 时代 中国 特色 社会主义 伟大 胜利 ， 为 实现 中华民族 伟大 复兴 的 中国 梦 不懈 奋斗\n",
      "清华分词器:\n",
      " 大会 的 主题 是 ： 不 忘 初心 ， 牢记 使命 ， 高举 中国 特色 社会主义 伟大 旗帜 ， 决胜 全面 建成 小康 社会 ， 夺取 新 时代 中国 特色 社会主义 伟大 胜利 ， 为 实现 中华民族 伟大 复兴 的 中国 梦 不 懈 奋斗\n",
      "哈工大分词器:\n",
      " \n",
      "中科院分词器:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 单个样本输入，输入为Unicode编码的字符串\n",
    "text = \"大会的主题是：不忘初心，牢记使命，高举中国特色社会主义伟大旗帜，决胜全面建成小康社会，夺取新时代中国特色社会主义伟大胜利，为实现中华民族伟大复兴的中国梦不懈奋斗\"\n",
    "# pattern = re.compile(r'[^\\u4e00-\\u9fa5]+')\n",
    "\n",
    "# def eliminate_punctuation(sentence):\n",
    "#     return pattern.sub('', str(sentence))\n",
    "# text = eliminate_punctuation(text)\n",
    "print(\"结巴分词器:\\n\",\" \".join(jieba.cut(text))) # 默认是精准模式\n",
    "print(\"百度分词器:\\n\",\" \".join(lac.run(text)[0]))\n",
    "print(\"snownlp分词器:\\n\",\" \".join(SnowNLP(text).words)) \n",
    "thu_text =[i[0] for i in thu.cut(text,text=False)]\n",
    "print(\"清华分词器:\\n\",\" \".join(thu_text))\n",
    "\n",
    "# [['他', '叫', '汤姆', '去', '拿', '外衣', '。']]\n",
    "print(\"哈工大分词器:\\n\",\"\\t\".join(ltp_text))\n",
    "\n",
    "print(\"中科院分词器:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent = \"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索 解空间树。\"\n",
    "words = segmentor.segment(sent)\n",
    "print ('\\t'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词性标注结果：\n",
      "我 r\n",
      "爱 v\n",
      "中国共产党 ORG\n",
      "分词结果: 我 爱 中国共产党\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱中国共产党\") #jieba默认模式\n",
    "jieba.enable_paddle() #启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "words = pseg.cut(\"我爱中国共产党\",use_paddle=True) #paddle模式\n",
    "print(\"词性标注结果：\")\n",
    "jieba_list = []\n",
    "for word, flag in words: # 必须使用此格式\n",
    "    jieba_list.append(word)\n",
    "    print('%s %s' % (word, flag))\n",
    "print(\"分词结果:\",\" \".join(jieba_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jiagu情感分析结果：\n",
      "('positive', 0.9701360293528393)\n",
      "('negative', 0.9384875029828238)\n",
      "SnowNLP情感分析结果：\n",
      "海底捞小火锅特别好吃 0.9686900092378237\n",
      "下次再也不买了 0.29108072363212245\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "print(\"Jiagu情感分析结果：\")\n",
    "text1 = '海底捞小火锅特别好吃'\n",
    "text2 = '下次再也不买了'\n",
    "for i in [text1,text2]:\n",
    "    sentiment = jiagu.sentiment(i)\n",
    "    print(sentiment) \n",
    "print(\"SnowNLP情感分析结果：\")\n",
    "s1 = SnowNLP(text1)\n",
    "s2 = SnowNLP(text2)\n",
    "print(text1, s1.sentiments)\n",
    "print(text2, s2.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut(\"我爱中国共产党\")) == \" \".join(jieba.cut(\"我爱中国共产党\")).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '中国共产党']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \" \".join(jieba.cut(\"我爱中国共产党\")).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总', '现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文', '将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上', '百度深度学习中文情感分析工具Senta试用及在线测试', 'BERT相关论文、文章和代码资源汇总'], 1: ['自然语言处理工具包spaCy介绍', '情感分析是自然语言处理里面一个热门话题'], 2: ['深度学习实践：从零开始做电影评论文本情感分析']}\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "docs = [\n",
    "        \"百度深度学习中文情感分析工具Senta试用及在线测试\",\n",
    "        \"情感分析是自然语言处理里面一个热门话题\",\n",
    "        \"AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总\",\n",
    "        \"深度学习实践：从零开始做电影评论文本情感分析\",\n",
    "        \"BERT相关论文、文章和代码资源汇总\",\n",
    "        \"将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上\",\n",
    "        \"自然语言处理工具包spaCy介绍\",\n",
    "        \"现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文\"\n",
    "    ]\n",
    "cluster = jiagu.text_cluster(docs)\t\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 站 在 桥 上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "text0 = '你站在桥上看风景，看风景的人在楼上看你。明月装饰了你的窗子，你装饰了别人的梦'\n",
    "seg = pkuseg.pkuseg()           # 以默认配置加载模型\n",
    "text = seg.cut(text0)  # 进行分词\n",
    "print(\" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "import jieba\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "from LAC import LAC\n",
    "lac = LAC(mode='seg')\n",
    "\n",
    "import thulac\n",
    "thu1 = thulac.thulac(seg_only=True,filt=True)  #只是分词不进行词性标注\n",
    "\n",
    "import pkuseg\n",
    "seg = pkuseg.pkuseg(model_name=\"literature\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textthu = thu1.cut(text0, text=True)  #进行一句话分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\GECHEN~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.432 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba分词器: 你 站 在 桥 上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n",
      "清华分词器: 你 站 在 桥 上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n",
      "snownlp分词器: 你 站 在 桥 上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n",
      "baidu分词器: 你 站 在 桥上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n",
      "jiagu分词器: 你 站在 桥上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n",
      "北大分词器: 你 站 在 桥 上 看 风景 ， 看 风景 的 人 在 楼上 看 你 。 明月 装饰 了 你 的 窗子 ， 你 装饰 了 别人 的 梦\n"
     ]
    }
   ],
   "source": [
    "print(\"jieba分词器:\",\" \".join(jieba.cut(text0)))\n",
    "print(\"清华分词器:\",textthu)\n",
    "print(\"snownlp分词器:\",\" \".join(SnowNLP(text0).words))\n",
    "print(\"baidu分词器:\",\" \".join(lac.run(text0)))\n",
    "print(\"jiagu分词器:\",\" \".join(jiagu.seg(text0))) # jiagu和百度将桥上划到了一起\n",
    "print(\"北大分词器:\",\" \".join(seg.cut(text0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 情感分析对比 snownlp和jiagu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jiagu情感分析结果：\n",
      "('negative', 0.744602885987926)\n",
      "('negative', 0.9384875029828238)\n",
      "SnowNLP情感分析结果：\n",
      "海底捞小火锅令人回味无穷 0.9938809424330315\n",
      "下次再也不买了 0.29108072363212245\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "print(\"Jiagu情感分析结果：\")\n",
    "text1 = '海底捞小火锅令人回味无穷'\n",
    "text2 = '下次再也不买了'\n",
    "for i in [text1,text2]:\n",
    "    sentiment = jiagu.sentiment(i)\n",
    "    print(sentiment) \n",
    "print(\"SnowNLP情感分析结果：\")\n",
    "s1 = SnowNLP(text1)\n",
    "s2 = SnowNLP(text2)\n",
    "print(text1, s1.sentiments)\n",
    "print(text2, s2.sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['百度深度学习中文情感分析工具Senta试用及在线测试', '深度学习实践：从零开始做电影评论文本情感分析'], 1: ['现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文', 'BERT相关论文、文章和代码资源汇总', '情感分析是自然语言处理里面一个热门话题', 'AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总', '将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上'], 2: ['自然语言处理工具包spaCy介绍']}\n"
     ]
    }
   ],
   "source": [
    "# jiagu聚类\n",
    "import jiagu\n",
    "docs = ['百度深度学习中文情感分析工具Senta试用及在线测试',\n",
    "            '情感分析是自然语言处理里面一个热门话题',\n",
    "            'AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总',\n",
    "            '深度学习实践：从零开始做电影评论文本情感分析',\n",
    "            'BERT相关论文、文章和代码资源汇总',\n",
    "            '将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上',\n",
    "            '自然语言处理工具包spaCy介绍',\n",
    "            '现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文']\n",
    "cluster = jiagu.text_cluster(docs)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['百度深度学习中文情感分析工具Senta试用及在线测试', '深度学习实践：从零开始做电影评论文本情感分析']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文',\n",
       " 'BERT相关论文、文章和代码资源汇总',\n",
       " '情感分析是自然语言处理里面一个热门话题',\n",
       " 'AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总',\n",
       " '将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['自然语言处理工具包spaCy介绍']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# snownlp，功能示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jiagu 功能示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jieba + gensim 功能示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baidu 功能示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清华 功能示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
